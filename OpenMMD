{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"OpenMMD","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMlaU8j33Y4ScjNEFFQXJnu"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"P-mbUWY56ZWF"},"source":["# OpenMMD\n","\n","[![language](https://img.shields.io/badge/Language-Python-blue.svg)](https://img.shields.io/badge/Language-Python-blue.svg)\n","\n","[OpenMMD](https://github.com/peterljq/OpenMMD) represents the OpenPose-Based Deep-Learning project that can directly **convert real-person videos to the motion of animation models (i.e. Miku, Anmicius)**. OpenMMD can be referred as OpenPose + MikuMikuDance (MMD). In short, you record a piece of video with human motions, through this project you will see a 3D model acting the same motions as what you do in the video.\n","<p align=\"center\">\n","<img src=\"https://raw.githubusercontent.com/peterljq/OpenMMD/master/Readme%20Materials/OpenMMD-Anmicius-Static.jpg\" width=\"240\">\n","</p>\n","<p align=\"center\">3D model example: Anmicius</p>\n","\n","- [OpenPose](https://github.com/CMU-Perceptual-Computing-Lab/openpose) is the first real-time multi-person system proposed by Carnegie Mellon University used to jointly detect human body key points on single images or videos. \n","- [MMD](https://sites.google.com/view/evpvp/) is a freeware animation program that lets users animate and create 3D animated movies using 3D models like Miku and Anmicius.\n","- OpenPose and MMD are only the \"entrance\" and \"exit\" of the application box. There are three intermediate pre-trained Deep Learning Models in the box to process and convert formatted data. They are stated in the Features section below.\n","\n","## Features\n","The project implements multiple Deep Learning Models as a sequential chain. The output of the previous model will be fed as the input of the following. Some implementations are the edited version of the original for better performance in the application.\n","- **Functionality**:\n","    - **3D Single-person Key Points Detection (OpenPose)**:\n","        - **Proposed by Gines Hidalgo, Zhe Cao, Tomas Simon, Shih-En Wei, Hanbyul Joo, and Yaser Sheikh at CVPR 2017**.\n","        - Recoded real-person video input and JSON files collections of motion key points as the output.\n","    - **Strong Baseline for 3D Human Pose Estimation**:\n","        - **Proposed by Julieta Martinez, Rayat Hossain, Javier Romero, James J. Little In ICCV, 2017**. An effective baseline for 3d human pose estimation.\n","        - Combining all the key points JSON files to a continuous sequence with strong baselines.\n","    - **Unsupervised Adversarial Learning of 3D Human Pose from 2D Joint Locations** (Newly added feature. Under testing.):\n","        - **Proposed by Yasunori Kudo, Keisuke Ogaki, Yusuke Matsui, Yuri Odagiri at CVPR 2018**. The task of 3D human pose estimation from a single image can be divided into two parts: (1) 2D human joint detection from the image and (2) estimating a 3D pose from the 2D joints.\n","        - Implemented by @DwangoMediaVillage to fit to VMD format. Use of GAN will significantly improve the performance during the converting process than what achived by using the baseline methods.\n","    - **Video Depth Prediction**:\n","        - **Proposed by Iro Laina and Christian Rupprecht at the IEEE International Conference on 3D Vision 2016**. FCRN: Deeper Depth Prediction with Fully Convolutional Residual Networks.\n","        - Estimation of depth for objects, backgrounds and the moving person in the video (e.g. dancer).\n","    - **Human Motion Key Points to VMD Motion Files for MMD Build**:\n","        - **Proposed by Denis Tome, Chris Russell and Lourdes Agapito at CVPR 2017**. Convolutional 3D Pose Estimation from a single image.\n","        - Edited by @miu200521358 to output VMD files so that the formatted result can be directly fed to MMD for generating animated dancing movies.\n","- **Input**: videos of common formats (AVI, WAV, MOV) or images of common formats (PNG, JPG), \n","- **Output**: Animations or Posetures of 3D models (e.g. Miku Dancing)\n","- **OS**: Windows (8, 10), MacOS (2017 Released Version)\n","\n","## Example Presentation\n","### I. Record a piece of real-person motion video\n","<p align=\"center\">\n","    <img src=\"https://github.com/peterljq/OpenMMD/blob/master/Readme%20Materials/Real-person%20Crop.gif?raw=true\", width=\"160\">\n","</p>\n","\n","### II. Extraction of 3D keypoints with Continuous Baselines\n","<p align=\"center\">\n","    <img src=\"https://raw.githubusercontent.com/peterljq/OpenMMD/master/Readme%20Materials/OpenMMD_smoothing.gif\" width=\"320\">\n","</p>\n","\n","### III. Video Depth Prediction\n","<p align=\"center\">\n","    <img src=\"https://github.com/peterljq/OpenMMD/blob/master/Readme%20Materials/OpenMMD_depth.gif?raw=true\" width=\"280\">\n","</p>\n","\n","### IV. Output VMD files and construct to MMD animations\n","<p align=\"center\">\n","    <img src=\"https://github.com/peterljq/OpenMMD/blob/master/Readme%20Materials/OpenMMD-Anmicius.gif?raw=true\" width=\"240\">\n","</p>\n","\n","\n","## Installation and User Guidelines\n","**Download the full pack**: Note that the full application is about 5GB. That is mainly because the large-size parameters of the pre-trained deep learning models. Download the whole pack contains the **pre-trained models with optimized parameters and corresponding compilable codes**. \n"]},{"cell_type":"code","metadata":{"id":"aLjIKfeU7_wa"},"source":["# Import PyDrive and associated libraries.\n","# This only needs to be done once per notebook.\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","# Authenticate and create the PyDrive client.\n","# This only needs to be done once per notebook.\n","auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)\n","\n","# Download a file based on its file ID.\n","#\n","# A file ID looks like: laggVyWshwcyP6kEI-y_W3P8D26sz\n","file_id = '197CWR_aOAd4vNflhWlxWmRVsAmeG5S1F'\n","downloaded = drive.CreateFile({'id': file_id})\n","print('Downloaded content \"{}\"'.format(downloaded.GetContentString()))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6o8l2Ri-8Y0B"},"source":["**Follow the instruction to begin your first animation**: \n","- Record a piece of video contains human motions. Satisfy all the prerequisite libraries stated below.\n","- After downloading, firstly **activate tensorflow** environment in the terminal of anaconda.\n","- Run OpenposeVideo.bat and follow the pop-out instructions.\n","- Then proceed to the 3d-pose-baseline-vmd folder and run OpenposeTo3D.bat. Follow the pop-out instructions.\n","- After that, proceed to the FCRN-DepthPrediction-vmd folder and run VideoToDepth.dat.\n","- Finally, proceed to the VMD-3d-pose-baseline-multi folder and run 3DToVMD.bat. You will get the vmd file.\n","- VMD files are 3D animation file used by **MikuMikuDance**, a program used to create dance animation movies. Open your MikuMikuDance and input the VMD file.\n","- You will see your Miku begin acting the same motions as that in your recorded video.\n","**Tutorial in Chinese (中文教程)**: Developers that can understand Chinese are encouraged to read the tutorial written by [@mrzjy](https://github.com/mrzjy) on Bilibili Articles: [Click Here](https://www.bilibili.com/read/cv2835857). This tutorial covers how to install and run the OpenMMD. The tutorial also introduces some common issues of OpenMMD. \n"]},{"cell_type":"markdown","metadata":{"id":"dm922t198i_B"},"source":["## Library Dependencies"]},{"cell_type":"markdown","metadata":{"id":"wSy2U8bu8ux7"},"source":["- OpenCV and relevance"]},{"cell_type":"code","metadata":{"id":"nUYnzf0q8lmQ"},"source":["!pip install opencv-python\n","!pip install numpy\n","!pip install matplotlib"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pZXYRde18rkC"},"source":["- Tensorflow and h5py. Please implement them in **anaconda**."]},{"cell_type":"code","metadata":{"id":"RVmhr7v981kk"},"source":["!pip install tensorflow\n","!conda create -n tensorflow pip python=3.6\n","activate tensorflow\n","!pip install --ignore-installed --upgrade tensorflow\n","!conda install h5py\n","!conda install keras"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MsEcdJEg88bn"},"source":["- Other libraries includes: "]},{"cell_type":"code","metadata":{"id":"1s9hl1bP89jc"},"source":["!pip install python-dateutil\n","!pip install pytz\n","!pip install pyparsing\n","!pip install six\n","!pip install imageio"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oGLus4bI9CeG"},"source":["## Special Thanks\n","I would like to give special thanks for the contribution from [@zhangxinyi0106](https://github.com/zhangxinyi0106). As an expert in Photoshop and Video Processing, He offered great help in recording and processing the sample images and videos. He also offered nice ideas on how to improve video smoothing performance.\n","\n","I would like to say special thanks to [@miu200521358](https://github.com/miu200521358) who provides a series of detailed tutorials on OpenPose and relevant models. That really promotes my implementation progress. His versions of model implementations are also of great help. I learn a lot from his instructions on anaconda and tensorflow.\n","\n","I would like to give special thanks to [@mrzjy](https://github.com/mrzjy) from Bilibili who writes a very detailed Chinese tutorial on how to install and run OpenMMD. This tutorial covers detailed steps and possible bugs when using the project. The quality of the tutorial is very high. 哔哩哔哩(゜-゜)つロ干杯!\n","\n","## Feedback\n","This project is an open source project. Let me know if:\n","\n","1. you find videos or images conversion does not work well.\n","2. you know how to speed up or improve the performance of 3D converting.\n","3. you have suggestions about future possible functionalities.\n","4. and anything you want to ask...\n","\n","Just comment on GitHub or make a pull request and I will answer as soon as possible!\n","\n","If you appreciate the project, please kindly star it. :D Feel free to download and develop your own 3D animations.\n","\n","Thank you for your time! "]}]}